{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 100.816 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 8.0 seconds.\n",
      "======\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>] 34149/34149, 2.9 task/s, elapsed: 11897s, ETA:     0s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "%matplotlib inline\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "import mmengine\n",
    "import os\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "from nuscenes.utils.data_classes import RadarPointCloud\n",
    "\n",
    "nusc = NuScenes(version='v1.0-trainval', dataroot='/opt/zhh/nuscenes/', verbose=True)\n",
    "camera_types = [\n",
    "            'CAM_FRONT',\n",
    "            'CAM_FRONT_RIGHT',\n",
    "            'CAM_FRONT_LEFT',\n",
    "            'CAM_BACK',\n",
    "            'CAM_BACK_LEFT',\n",
    "            'CAM_BACK_RIGHT',\n",
    "        ]\n",
    "RADARS_FOR_CAMERA = {\n",
    "            'CAM_FRONT_LEFT': [\"RADAR_FRONT_LEFT\", \"RADAR_FRONT\"],\n",
    "            'CAM_FRONT_RIGHT': [\"RADAR_FRONT_RIGHT\", \"RADAR_FRONT\"],\n",
    "            'CAM_FRONT': [\"RADAR_FRONT_RIGHT\", \"RADAR_FRONT_LEFT\", \"RADAR_FRONT\"],\n",
    "            'CAM_BACK_LEFT': [\"RADAR_BACK_LEFT\", \"RADAR_FRONT_LEFT\"],\n",
    "            'CAM_BACK_RIGHT': [\"RADAR_BACK_RIGHT\", \"RADAR_FRONT_RIGHT\"],\n",
    "            'CAM_BACK': [\"RADAR_BACK_RIGHT\", \"RADAR_BACK_LEFT\"]}\n",
    "   \n",
    "rad_root = \"/opt/zhh/nuscenes/radar_map/\"\n",
    "num_sweeps=3\n",
    "for sample in mmengine.track_iter_progress(nusc.sample):\n",
    "    for cam in camera_types:\n",
    "        radars_related =RADARS_FOR_CAMERA[cam]\n",
    "        radar_tokens = [sample['data'][_] for _ in radars_related]\n",
    "        radar_records = [nusc.get(\"sample_data\", _) for _ in radar_tokens]\n",
    "        camera_token = sample['data'][cam]\n",
    "        camera_record = nusc.get(\"sample_data\", camera_token)\n",
    "        img_path=os.path.join(nusc.dataroot, camera_record['filename'])\n",
    "        # im = Image.open(img_path)\n",
    "        lidar_token = sample['data']['LIDAR_TOP']\n",
    "        lidar_record = nusc.get(\"sample_data\", lidar_token)\n",
    "        # Get current point cloud\n",
    "        point_cloud_list = []\n",
    "        for i in range(len(radar_records)):\n",
    "            # Original version: all radar points are mapped to LIDAR_TOP\n",
    "            radar_record = radar_records[i]\n",
    "            point_clouds, times = RadarPointCloud.from_file_multisweep(nusc, sample,\n",
    "                                                                            radar_record[\"channel\"], \"LIDAR_TOP\",\n",
    "                                                                            nsweeps=num_sweeps)\n",
    "            point_cloud_list.append(point_clouds.points)\n",
    "        point_clouds = RadarPointCloud(np.concatenate(tuple(point_cloud_list), axis=1))\n",
    "\n",
    "        #增加高度\n",
    "        a=point_clouds.points\n",
    "        a_1=np.tile(a,(1,5))\n",
    "        a_1[2,(a.shape[1]):2*(a.shape[1])]+=0.2\n",
    "        a_1[2,2*(a.shape[1]):3*(a.shape[1])]+=0.5\n",
    "        a_1[2,3*(a.shape[1]):4*(a.shape[1])]+=-0.2\n",
    "        a_1[2,4*(a.shape[1]):5*(a.shape[1])]+=1\n",
    "        a_1[2,5*(a.shape[1]):6*(a.shape[1])]+=-0.3\n",
    "        #加高开关\n",
    "        point_clouds.points=a_1\n",
    "\n",
    "\n",
    "\n",
    "        point_cloud_coords = point_clouds.points[:3, :].transpose()\n",
    "        kdtree = KDTree(point_cloud_coords)\n",
    "        filtered_point_indices = []\n",
    "        for x in sample[\"anns\"]:\n",
    "            ann_record=nusc.get('sample_annotation',x)\n",
    "            sample_record = nusc.get('sample', ann_record['sample_token'])\n",
    "            lidar = sample_record['data']['LIDAR_TOP']\n",
    "            data_path, grt_box, camera_intrinsic = nusc.get_sample_data(lidar, selected_anntokens=[x])\n",
    "            query=grt_box[0].center\n",
    "            width, length, height = grt_box[0].wlh\n",
    "            r_max=np.sqrt((width/2)**2+(length/2)**2+(height/2)**2)\n",
    "            indices = kdtree.query_ball_point(query,r_max+1)\n",
    "            filtered_point_indices.extend(indices)\n",
    "        #去除重复的点云索引\n",
    "        filtered_point_indices = list(set(filtered_point_indices))\n",
    "        # 提取过滤后的点云数据\n",
    "        filtered_point_cloud = point_clouds.points[:,filtered_point_indices]\n",
    "        #过滤开关\n",
    "        point_clouds.points=filtered_point_cloud\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # First step: transform the point-cloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "        cs_record = nusc.get('calibrated_sensor', lidar_record['calibrated_sensor_token'])\n",
    "        point_clouds.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "        point_clouds.translate(np.array(cs_record['translation']))\n",
    "        # Second step: transform to the global frame.\n",
    "        poserecord = nusc.get('ego_pose', lidar_record['ego_pose_token'])\n",
    "        point_clouds.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "        point_clouds.translate(np.array(poserecord['translation']))\n",
    "        # Third step: transform into the ego vehicle frame for the timestamp of the image.\n",
    "        poserecord = nusc.get('ego_pose', camera_record['ego_pose_token'])\n",
    "        point_clouds.translate(-np.array(poserecord['translation']))\n",
    "        point_clouds.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "        # Fourth step: transform into the camera.\n",
    "        cs_record = nusc.get('calibrated_sensor', camera_record['calibrated_sensor_token'])\n",
    "        point_clouds.translate(-np.array(cs_record['translation']))\n",
    "        point_clouds.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "        # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "                # Grab the depths (camera frame z axis points away from the camera).\n",
    "\n",
    "        #为了便于后面投影图像后拼接特征，先把点特征搞出来，所以只提【5，8，9】即rcs,补偿速度\n",
    "        points_feature=point_clouds.points[[0,1,2,5,8,9],:]\n",
    "\n",
    "\n",
    "        filename, ext = os.path.splitext(img_path)\n",
    "        point_path=filename + '.npy'\n",
    "        path_parts = point_path.split('/')\n",
    "        rpath_new= rad_root + '/'.join(path_parts[4:])\n",
    "        np.save(rpath_new, points_feature)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
